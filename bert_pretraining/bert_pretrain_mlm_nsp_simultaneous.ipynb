{"cells":[{"cell_type":"markdown","metadata":{"id":"udDENNOTBWRq"},"source":["# Pretrain Bert on MLM and NSP Simultaneously"]},{"cell_type":"markdown","metadata":{"id":"GIfOBEuTpRY-"},"source":["Ref:\n","\n","(i) https://stackoverflow.com/questions/70122842/bert-pre-training-mlm-nsp?rq=1\n","\n","\n","(ii) https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp\n","\n","\n","(iii) https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13228,"status":"ok","timestamp":1639307141545,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"ca5CZJR2BYOI","outputId":"9d521556-33ce-4b62-d069-a7edfaa42f84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 4.3 MB/s \n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 42.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 555 kB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 57.7 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 66.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 58.5 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 53.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 51.6 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 60.9 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 59.8 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0 xxhash-2.0.2 yarl-1.7.2\n"]}],"source":["!pip install datasets transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3yXkAk84QAZ"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","from transformers import (\n","    BertTokenizer,\n","    BertTokenizerFast,\n","    BertConfig, \n","    BertForPreTraining, \n","    TextDatasetForNextSentencePrediction,\n","    DataCollatorForLanguageModeling,\n","    Trainer, \n","    TrainingArguments\n",")\n","import torch\n","from datasets import load_dataset, concatenate_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":665,"status":"ok","timestamp":1639307151503,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"hJPDszCdkvSL","outputId":"767d8c8c-c9ec-4d9e-9116-cad7e89bf21e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["# For sentence tokenization\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bb8hKfFXBLrj"},"outputs":[],"source":["# CONFIGS\n","\n","RANDOM_SEED=37\n","\n","DATASET_LIMIT = 20_000\n","\n","# MODEL_MAX_LEN = 256\n","NSP_DATESET_PATH = 'nsp.txt'\n","MLM_TRAIN_DATESET_PATH = 'mlm_train.txt'\n","MLM_TEST_DATESET_PATH = 'mlm_test.txt'\n","MLM_MASKING_PROB = .15\n","\n","MODEL_NAME = \"bert-base-uncased\"\n","# MODEL_NAME = \"bert-base-multilingual-uncased\"\n","\n","MODEL_SAVE_PATH = MODEL_NAME"]},{"cell_type":"markdown","metadata":{"id":"pJouGi0p46Ui"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["39d2780e6877487280c05bea52afef79","9d8ae9730ef84870a91f94abc73364a1","164cb74de8fb4e3c9d1d717d662fba26","9cfea2573fad430ea4977595c8ec3674","f89a11d0f42545b0aa8373753f09e3eb","1d29e9b338604e8c963337785f087654","7989cd30e34d4050bb32cbf804b42f8a","dd5b6f345abc44aaa35b8bf2bd2f1d78","c66ead0a55a44f1c90e03c0c99cf91a0","ecfcb55db45649b7bba8fde8797ea100","b215360681b9412dba739f1c0f13e031","feb21738206d44e4af71fe200b8cb934","d566bc16a7da478ba2639ab05caeca54","782e3afa9a134c48ab363e8a1084076d","b64a151c71c9457aa9e293cab46875cb","2ec07736b4c6480096230df8c6d746ac","85da9fb0d08b46d79196b9f9fa40da69","7007553a57264e0c93e74d14b1b4d75c","07dffb53be614c529c81610513341d8f","e939a95a1b164da9ae39e25ce4ff842f","6fce4dedd7994808b879509645b0b2a2","decc091e573142c5897e5d9385556cf8","3b72164d26964571a858c48b15f8113f","6b5b176eb9854930ab50192bba5defa2","d3bfdc4edc9f4e19b8ac4fdfcec597f3","a3f5cf9ffbf84a77a3f5faf9527c7369","54f67b878c4747a5b1b607dcc17eadd1","07b727c1b5e34a0cb128244e50a3bce7","30841fc371174c2f8a025e0f5d1d0cf6","5c4c5eb0f0c248b3850612203ba60676","b26b5bd4fdb147bfb1c19bb8901178ff","9e8d77cecf6549a4930a2724025b2deb","fa7214d5010c44f8bc70c82a9832ce71","a8a8604af96145beb1d24c8dc4af2ae0","2c06bea1b3424abf9129c853e0649aac","5c4693c98ade4f51bcb451ad5167c8f3","524b5990cab24df3b11c9c97ffe776ad","8fb09da40e9744249824cb534ecfc9a4","37090717df1c44808ecc7cb00ff21495","721db08dc1f34f649a352a6c05f1ad82","cd8bebe3752042b7a9f044a7493b4a8f","4fe51e17379440f0ad1b079d402e2a87","7c2b665a2dbf430493d3a6573fb7d22c","ef34e9d4f42b4513a38f30eda6a83bb5"]},"executionInfo":{"elapsed":253958,"status":"ok","timestamp":1639307405457,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"F3O2Q0AH4zif","outputId":"73f5517f-58c5-4340-e988-cb6c961f19f0"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39d2780e6877487280c05bea52afef79","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"feb21738206d44e4af71fe200b8cb934","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/932 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset cc_news/plain_text (download: 805.98 MiB, generated: 1.88 GiB, post-processed: Unknown size, total: 2.67 GiB) to /root/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/ae469e556251e6e7e20a789f93803c7de19d0c4311b6854ab072fecb4e401bd6...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b72164d26964571a858c48b15f8113f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/845M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8a8604af96145beb1d24c8dc4af2ae0","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset cc_news downloaded and prepared to /root/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/ae469e556251e6e7e20a789f93803c7de19d0c4311b6854ab072fecb4e401bd6. Subsequent calls will reuse this data.\n"]}],"source":["# wiki = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\")\n","# bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n","# print(wiki.column_names, bookcorpus.column_names)\n","# # ['title', 'text'] ['text']\n","\n","# wiki.remove_columns_(\"title\")\n","# bert_dataset = concatenate_datasets([wiki, bookcorpus])\n","\n","\n","dataset = load_dataset(\"cc_news\", split=\"train\")\n","\n","bert_dataset = dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1639307405459,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"ICxVkAg8Feog","outputId":"156896ea-f9cd-463b-9088-582ec4e00a6c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n","    num_rows: 708241\n","})"]},"metadata":{},"execution_count":6}],"source":["bert_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1639307405460,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"YmNOvjaCIRd4","outputId":"4abc4d80-7ef9-4cc6-92af-cea5475bd067"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'date': '2017-12-11 20:19:05',\n"," 'description': \"There's a surprising twist to Regina Willoughby's last season with\\xa0Columbia City Ballet: It's also her 18-year-old daughter Melina's first season with the company.\",\n"," 'domain': 'www.pointemagazine.com',\n"," 'image_url': 'https://pointe-img.rbl.ms/simage/https%3A%2F%2Fassets.rbl.ms%2F16807693%2F980x.png/2000%2C2000/3VnhNGWp75K4SwMx/img.png',\n"," 'text': 'There\\'s a surprising twist to Regina Willoughby\\'s last season with Columbia City Ballet: It\\'s also her 18-year-old daughter Melina\\'s first season with the company. Regina, 40, will retire from the stage in March, just as her daughter starts her own career as a trainee. But for this one season, they\\'re sharing the stage together.\\nPerforming Side-By-Side In The Nutcracker\\nRegina and Melina are not only dancing in the same Nutcracker this month, they\\'re onstage at the same time: Regina is doing Snow Queen, while Melina is in the snow corps, and they\\'re both in the Arabian divertissement. \"It\\'s very surreal to be dancing it together,\" says Regina. \"I don\\'t know that I ever thought Melina would take ballet this far.\"\\nLeft: Regina and Melina with another company member post-snow scene in 2003. Right: The pair post-snow scene in 2017 (in the same theater)\\nKeep reading at dancemagazine.com.',\n"," 'title': 'Daughter Duo is Dancing in The Same Company',\n"," 'url': 'http://www.pointemagazine.com/mother-daughter-duo-dancing-2516681965.html'}"]},"metadata":{},"execution_count":7}],"source":["bert_dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"nab0NUnRA0g7"},"source":["### For MLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnO75nsg3HF_"},"outputs":[],"source":["# def split_string(str, limit, sep=\" \"):\n","#     \"\"\"\n","#     Split a long string into list of substrings each of\n","#     which has length less than the given limit.\n","#     \"\"\"\n","#     words = str.split()\n","#     words = list(filter(lambda x: len(x)<limit, words))\n","#     if max(map(len, words)) > limit:\n","#         raise ValueError(\"limit is too small\")\n","#     res, part, others = [], words[0], words[1:]\n","#     for word in others:\n","#         if len(sep)+len(word) > limit-len(part):\n","#             res.append(part)\n","#             part = word\n","#         else:\n","#             part += sep+word\n","#     if part:\n","#         res.append(part)\n","#     return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19X8FP1UAzd7"},"outputs":[],"source":["# def dataset_to_text(dataset, output_filename):\n","#   \"\"\"\n","#   Write dataset to file\n","#   \"\"\"\n","#   with open(output_filename, \"w\") as f:\n","#     for document in dataset[\"text\"]:\n","#       if len(document)<=MODEL_MAX_LEN:\n","#         # If document is less than the max model length\n","#         print(document, file=f)\n","#       else:\n","#         splitted_substrings = split_string(document, MODEL_MAX_LEN)\n","#         for substring in splitted_substrings:\n","#           print(substring, file=f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J12PTgQ1BJZ-"},"outputs":[],"source":["# # split the dataset into training (90%) and testing (10%)\n","# d = bert_dataset.train_test_split(test_size=0.1, seed=RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JF4BjXRBHDR"},"outputs":[],"source":["# # save the training set to train.txt\n","# dataset_to_text(d[\"train\"][:DATASET_LIMIT], MLM_TRAIN_DATESET_PATH)\n","# # save the testing set to test.txt\n","# dataset_to_text(d[\"test\"][:DATASET_LIMIT//10], MLM_TEST_DATESET_PATH)"]},{"cell_type":"markdown","metadata":{"id":"W8Ee0_jCBXmf"},"source":["### For NSP\n","(1) One sentence per line. \n","\n","(2) Blank lines between documents\n","\n","\n","ref: https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"]},{"cell_type":"code","source":["with open(NSP_DATESET_PATH, \"w\") as f:\n","  for document in bert_dataset[:DATASET_LIMIT][\"text\"]:\n","    # replace paragraph changes with fullstop for sentence segmentation\n","    document = document.replace('\\n', '')\n","    for sentence in sent_tokenize(document):\n","      sentence = sentence.strip()\n","      if sentence != '':\n","          print(sentence, file=f)\n","    # line break for each document\n","    print('', file=f)"],"metadata":{"id":"jkXibp0_Unz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sg9SPFfqBZ_F"},"outputs":[],"source":["# with open(NSP_DATESET_PATH, \"w\") as f:\n","#   for document in bert_dataset[:DATASET_LIMIT][\"text\"]:\n","#     # replace paragraph changes with fullstop for sentence segmentation\n","#     document = document.replace('\\n', '')\n","#     for sentence in sent_tokenize(document):\n","#       sentence = sentence.strip()\n","#       if sentence != '':\n","#         if len(sentence)<=MODEL_MAX_LEN:\n","#           # If string is less than the max model length\n","#           print(sentence, file=f)\n","#         else:\n","#           splitted_substrings = split_string(sentence, MODEL_MAX_LEN)\n","#           for substring in splitted_substrings:\n","#             print(substring, file=f)\n","#     # line break for each document\n","#     print('', file=f)\n","\n","\n","# Remove final line breaks  \n","with open(NSP_DATESET_PATH) as f_input:\n","    data = f_input.read().rstrip('\\n')\n","\n","with open(NSP_DATESET_PATH, 'w') as f_output:    \n","    f_output.write(data)"]},{"cell_type":"code","source":["# Remove final line breaks\n","line_len = []  \n","with open(NSP_DATESET_PATH) as f_input:\n","    data = f_input.read()\n","    for line in data.split('\\n'):\n","        line_len.append(len(line))\n","\n","print(max(line_len))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVoAqXVXCmRg","executionInfo":{"status":"ok","timestamp":1639307782756,"user_tz":-660,"elapsed":1130,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"}},"outputId":"4afeedbf-c4b9-4230-c4c6-bc86639eed5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10815\n"]}]},{"cell_type":"code","source":["lines = data.split('\\n')"],"metadata":{"id":"TiDu4M9SDzAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"cluf2Th_EmWz","executionInfo":{"status":"ok","timestamp":1639307785207,"user_tz":-660,"elapsed":20,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"}},"outputId":"1caa212b-5208-4819-c612-eb54e7533e6d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Joining Stafford are NYCB resident choreographer and soloist Justin Peck and ballet masters Craig Hall and Rebecca Krohn, both former dancers with the company.'"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"Z3fYo2lH55nK"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6566,"status":"ok","timestamp":1639309056839,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"sR_mjKKe4-Le","outputId":"f00f9ed4-2fcf-4caf-aeef-04603eaeb271"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}],"source":["bert_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n","# bert_tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME, max_len=512)"]},{"cell_type":"code","source":["dir(bert_tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emuvx_U1blRa","executionInfo":{"status":"ok","timestamp":1639309010262,"user_tz":-660,"elapsed":633,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"}},"outputId":"a3fa7ed3-71ba-4e44-8d15-901a4a76cf99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['SPECIAL_TOKENS_ATTRIBUTES',\n"," '__annotations__',\n"," '__call__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__len__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_add_tokens',\n"," '_additional_special_tokens',\n"," '_batch_encode_plus',\n"," '_batch_prepare_for_model',\n"," '_bos_token',\n"," '_cls_token',\n"," '_convert_id_to_token',\n"," '_convert_token_to_id',\n"," '_convert_token_to_id_with_added_voc',\n"," '_create_or_get_repo',\n"," '_create_trie',\n"," '_decode',\n"," '_decode_use_source_tokenizer',\n"," '_encode_plus',\n"," '_eos_token',\n"," '_eventual_warn_about_too_long_sequence',\n"," '_from_pretrained',\n"," '_get_padding_truncation_strategies',\n"," '_get_repo_url_from_name',\n"," '_mask_token',\n"," '_pad',\n"," '_pad_token',\n"," '_pad_token_type_id',\n"," '_push_to_hub',\n"," '_save_pretrained',\n"," '_sep_token',\n"," '_tokenize',\n"," '_unk_token',\n"," 'add_special_tokens',\n"," 'add_tokens',\n"," 'added_tokens_decoder',\n"," 'added_tokens_encoder',\n"," 'additional_special_tokens',\n"," 'additional_special_tokens_ids',\n"," 'all_special_ids',\n"," 'all_special_tokens',\n"," 'all_special_tokens_extended',\n"," 'as_target_tokenizer',\n"," 'basic_tokenizer',\n"," 'batch_decode',\n"," 'batch_encode_plus',\n"," 'bos_token',\n"," 'bos_token_id',\n"," 'build_inputs_with_special_tokens',\n"," 'clean_up_tokenization',\n"," 'cls_token',\n"," 'cls_token_id',\n"," 'convert_ids_to_tokens',\n"," 'convert_tokens_to_ids',\n"," 'convert_tokens_to_string',\n"," 'create_token_type_ids_from_sequences',\n"," 'decode',\n"," 'deprecation_warnings',\n"," 'do_basic_tokenize',\n"," 'do_lower_case',\n"," 'encode',\n"," 'encode_plus',\n"," 'eos_token',\n"," 'eos_token_id',\n"," 'from_pretrained',\n"," 'get_added_vocab',\n"," 'get_special_tokens_mask',\n"," 'get_vocab',\n"," 'ids_to_tokens',\n"," 'init_inputs',\n"," 'init_kwargs',\n"," 'is_fast',\n"," 'mask_token',\n"," 'mask_token_id',\n"," 'max_len_sentences_pair',\n"," 'max_len_single_sentence',\n"," 'max_model_input_sizes',\n"," 'model_input_names',\n"," 'model_max_length',\n"," 'name_or_path',\n"," 'num_special_tokens_to_add',\n"," 'pad',\n"," 'pad_token',\n"," 'pad_token_id',\n"," 'pad_token_type_id',\n"," 'padding_side',\n"," 'prepare_for_model',\n"," 'prepare_for_tokenization',\n"," 'prepare_seq2seq_batch',\n"," 'pretrained_init_configuration',\n"," 'pretrained_vocab_files_map',\n"," 'push_to_hub',\n"," 'sanitize_special_tokens',\n"," 'save_pretrained',\n"," 'save_vocabulary',\n"," 'sep_token',\n"," 'sep_token_id',\n"," 'slow_tokenizer_class',\n"," 'special_tokens_map',\n"," 'special_tokens_map_extended',\n"," 'tokenize',\n"," 'tokens_trie',\n"," 'truncate_sequences',\n"," 'unique_no_split_tokens',\n"," 'unk_token',\n"," 'unk_token_id',\n"," 'verbose',\n"," 'vocab',\n"," 'vocab_files_names',\n"," 'vocab_size',\n"," 'wordpiece_tokenizer']"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"JHr_URNB6BxM"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzWSEK3jNFFe"},"outputs":[],"source":["config = BertConfig(\n","    num_hidden_layers=4, \n","    num_attention_heads=4, \n",")\n","model = BertForPreTraining(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1639309064716,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"J0eJlKdYd1C1","outputId":"79bb3ede-d990-4815-8374-fa7d45c76e89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForPreTraining(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertPreTrainingHeads(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n","    )\n","    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":33}],"source":["device = torch.device('cpu')# and move our model over to the selected device\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"ohln30HMp3YY"},"source":["#### NSP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15492,"status":"ok","timestamp":1639309082354,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"1_s865NzLprN","outputId":"9fda3768-1491-45fd-d83e-4399144d8514"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:366: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Loading features from cached file cached_nsp_BertTokenizer_256_nsp.txt [took 15.486 s]\n"]}],"source":["dataset = TextDatasetForNextSentencePrediction(\n","    tokenizer=bert_tokenizer,\n","    file_path=NSP_DATESET_PATH,\n","    block_size = 256\n",")"]},{"cell_type":"markdown","metadata":{"id":"DbrMdMtZp4ui"},"source":["#### MLM\n","\n","\n","Use DataCollatorForLanguageModeling for masking and passing the labels that are generated from TextDatasetForNextSentencePrediction. DataCollatorForNextSentencePrediction has been removed, since it was doing the same thing with DataCollatorForLanguageModeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxyHZOxaMTgZ"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=bert_tokenizer, \n","    mlm=True,\n","    mlm_probability= MLM_MASKING_PROB\n",")"]},{"cell_type":"markdown","metadata":{"id":"AbKUWjGJMyPJ"},"source":["### Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1639309091553,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"},"user_tz":-660},"id":"-G1FLoSRMz-5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0432fff6-6a9a-490b-ab5f-9eda61deafa2"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["training_args = TrainingArguments(\n","    output_dir= \"results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size= 32,\n","    save_steps=1000,\n","    save_on_each_node=True,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpQXPLrK9AZG"},"outputs":[],"source":["# import torch\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"id":"JQqe-yHm6BCj","executionInfo":{"status":"error","timestamp":1639309094944,"user_tz":-660,"elapsed":2888,"user":{"displayName":"Reeya Sunuwarni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2QUZle9e41SdlBi1MpUCFloE_REsGxoGJJYKLcw=s64","userId":"17710346067698748010"}},"outputId":"f46b6c08-31f5-4bfa-96ed-2ac6071deb56"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 68283\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10670\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='10670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    5/10670 00:02 < 2:03:17, 1.44 it/s, Epoch 0.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-47b1a8d96bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1323\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, next_sentence_label, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         )\n\u001b[1;32m    999\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (627) must match the size of tensor b (512) at non-singleton dimension 1"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbZXLj_IM8_Y"},"outputs":[],"source":["trainer.save_model(MODEL_SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUU4TewrE1hY"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"bert_pretrain_mlm_nsp_simultaneous.ipynb","provenance":[],"authorship_tag":"ABX9TyNpGXT3ht+7ezEXFtD8wFbz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"39d2780e6877487280c05bea52afef79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9d8ae9730ef84870a91f94abc73364a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_164cb74de8fb4e3c9d1d717d662fba26","IPY_MODEL_9cfea2573fad430ea4977595c8ec3674","IPY_MODEL_f89a11d0f42545b0aa8373753f09e3eb"]}},"9d8ae9730ef84870a91f94abc73364a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"164cb74de8fb4e3c9d1d717d662fba26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d29e9b338604e8c963337785f087654","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7989cd30e34d4050bb32cbf804b42f8a"}},"9cfea2573fad430ea4977595c8ec3674":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dd5b6f345abc44aaa35b8bf2bd2f1d78","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1746,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1746,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c66ead0a55a44f1c90e03c0c99cf91a0"}},"f89a11d0f42545b0aa8373753f09e3eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ecfcb55db45649b7bba8fde8797ea100","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.38k/? [00:00&lt;00:00, 117kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b215360681b9412dba739f1c0f13e031"}},"1d29e9b338604e8c963337785f087654":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7989cd30e34d4050bb32cbf804b42f8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd5b6f345abc44aaa35b8bf2bd2f1d78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c66ead0a55a44f1c90e03c0c99cf91a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecfcb55db45649b7bba8fde8797ea100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b215360681b9412dba739f1c0f13e031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"feb21738206d44e4af71fe200b8cb934":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d566bc16a7da478ba2639ab05caeca54","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_782e3afa9a134c48ab363e8a1084076d","IPY_MODEL_b64a151c71c9457aa9e293cab46875cb","IPY_MODEL_2ec07736b4c6480096230df8c6d746ac"]}},"d566bc16a7da478ba2639ab05caeca54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"782e3afa9a134c48ab363e8a1084076d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85da9fb0d08b46d79196b9f9fa40da69","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7007553a57264e0c93e74d14b1b4d75c"}},"b64a151c71c9457aa9e293cab46875cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_07dffb53be614c529c81610513341d8f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":932,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":932,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e939a95a1b164da9ae39e25ce4ff842f"}},"2ec07736b4c6480096230df8c6d746ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6fce4dedd7994808b879509645b0b2a2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.04k/? [00:00&lt;00:00, 44.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_decc091e573142c5897e5d9385556cf8"}},"85da9fb0d08b46d79196b9f9fa40da69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7007553a57264e0c93e74d14b1b4d75c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07dffb53be614c529c81610513341d8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e939a95a1b164da9ae39e25ce4ff842f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6fce4dedd7994808b879509645b0b2a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"decc091e573142c5897e5d9385556cf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b72164d26964571a858c48b15f8113f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6b5b176eb9854930ab50192bba5defa2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d3bfdc4edc9f4e19b8ac4fdfcec597f3","IPY_MODEL_a3f5cf9ffbf84a77a3f5faf9527c7369","IPY_MODEL_54f67b878c4747a5b1b607dcc17eadd1"]}},"6b5b176eb9854930ab50192bba5defa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3bfdc4edc9f4e19b8ac4fdfcec597f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_07b727c1b5e34a0cb128244e50a3bce7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30841fc371174c2f8a025e0f5d1d0cf6"}},"a3f5cf9ffbf84a77a3f5faf9527c7369":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5c4c5eb0f0c248b3850612203ba60676","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":845131146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":845131146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b26b5bd4fdb147bfb1c19bb8901178ff"}},"54f67b878c4747a5b1b607dcc17eadd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9e8d77cecf6549a4930a2724025b2deb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 845M/845M [00:24&lt;00:00, 35.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa7214d5010c44f8bc70c82a9832ce71"}},"07b727c1b5e34a0cb128244e50a3bce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"30841fc371174c2f8a025e0f5d1d0cf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c4c5eb0f0c248b3850612203ba60676":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b26b5bd4fdb147bfb1c19bb8901178ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e8d77cecf6549a4930a2724025b2deb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fa7214d5010c44f8bc70c82a9832ce71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a8a8604af96145beb1d24c8dc4af2ae0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2c06bea1b3424abf9129c853e0649aac","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5c4693c98ade4f51bcb451ad5167c8f3","IPY_MODEL_524b5990cab24df3b11c9c97ffe776ad","IPY_MODEL_8fb09da40e9744249824cb534ecfc9a4"]}},"2c06bea1b3424abf9129c853e0649aac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c4693c98ade4f51bcb451ad5167c8f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_37090717df1c44808ecc7cb00ff21495","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_721db08dc1f34f649a352a6c05f1ad82"}},"524b5990cab24df3b11c9c97ffe776ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cd8bebe3752042b7a9f044a7493b4a8f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4fe51e17379440f0ad1b079d402e2a87"}},"8fb09da40e9744249824cb534ecfc9a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7c2b665a2dbf430493d3a6573fb7d22c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 707932/0 [03:44&lt;00:00, 3114.62 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef34e9d4f42b4513a38f30eda6a83bb5"}},"37090717df1c44808ecc7cb00ff21495":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"721db08dc1f34f649a352a6c05f1ad82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cd8bebe3752042b7a9f044a7493b4a8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4fe51e17379440f0ad1b079d402e2a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c2b665a2dbf430493d3a6573fb7d22c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef34e9d4f42b4513a38f30eda6a83bb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}